{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model and DataParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We need to make a model instance and check if we have multiple GPUs.\n",
    "+ If we have multiple GPU's, we can wrap our model using __nn.DataParallel__. Then we can put our model on GPUs by __model.to(device)__\n",
    "    + __device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")__\n",
    "    + __model = Model(input_size, output_size)__\n",
    "    + __if torch.cuda.device_count() > 1:__\n",
    "    + __model = nn.DataParallel(model)__\n",
    "    + __model.to(device)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __DataParallel__ splits your data automatically and sends job orders to multiple models on several GPUs. Agter each model finishes their job, DataParallel collects and merges the results before returning it to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TM",
   "language": "python",
   "name": "tm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
